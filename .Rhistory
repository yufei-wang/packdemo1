#     cl_test <- cl1 %>% filter(split == i)
#     cl_train$split <- NULL
#     cl_test$split <- NULL
#     yhat_star <- knn(train = data_train, test = data_test,
#                      cl = cl_train[,1, drop = TRUE], k = k_nn)
#     yhat_star1 <- data.frame(yhat_star)
#
#     cv_err_vec[i] = sum(yhat_star1 != cl_test)/30
#   }
#   class <- knn(train = train, test = train, cl = cl, k = k_nn)
#   cv_err <- mean(cv_err_vec)
#   output <- list(class,cv_err)
#   return(output)
# }
my_knn_cv <- function(train, cl, k_nn, k_cv){
set.seed(302)
n <- nrow(train)
# Split data in k_cv parts randomly
fold <- sample(rep(1:k_cv, length = n))
data <- data.frame(train, "split" = fold)
cl1 <- data.frame(cl,  "split" = fold)
cv_err_vec <- rep(NA, k_cv)
for (i in 1:k_cv) {
data_train <- data %>% filter(split != i)
data_test <- data %>% filter(split == i)
cl_train <- cl1 %>% filter(split != i)
cl_test <- cl1 %>% filter(split == i)
cl_train$split <- NULL
cl_test$split <- NULL
# predicts output class
yhat_star <- knn(train = data_train, test = data_test,
cl = cl_train[,1, drop = TRUE], k = k_nn)
#convert to data frame
yhat_star.df <- data.frame(yhat_star)
cv_err_vec[i] = sum(yhat_star.df != cl_test) / length(cl_test)
}
class <- knn(train = train, test = train, cl = cl, k = k_nn)
cv_err <- mean(cv_err_vec)
output <- list("class" = class, "cv_err" = cv_err)
return(output)
}
output_knn1 <- my_knn_cv(train = iris[, -5], cl = iris$Species,
k_nn=1, k_cv = 5)
output_knn5 <- my_knn_cv(train = iris[, -5], cl = iris$Species,
k_nn=5, k_cv = 5)
result_cv_err <- cbind(output_knn1[[2]], output_knn5[[2]])
colnames(result_cv_err) <- c("CV error (knn = 1)", "CV error (knn = 5)")
result_cv_err
# the cv error of k_nn = 5 is lower and cv error of k_nn = 1 is higher.
output_knn1[[1]]
output_knn5[[1]]
iris$Species
# the training set error of k_nn = 1 is 0 and the training set
# error of k_nn = 5 is higher.
library(dplyr)
library(class)
my_knn_cv <- function(train, cl, k_nn, k_cv){
set.seed(302)
fold <- sample(rep(1:k_cv, length = 150))
data <- data.frame(train, "split" = fold)
cl1 <- data.frame(cl,  "split" = fold)
my_pred <- matrix(NA, 30, 1)
cv_err_vec <- rep(NA, k_cv)
for (i in 1:k_cv) {
data_train <- data %>% filter(split != i)
data_test <- data %>% filter(split == i)
cl_train <- cl1 %>% filter(split != i)
cl_test <- cl1 %>% filter(split == i)
cl_train$split <- NULL
cl_test$split <- NULL
yhat_star <- knn(train = data_train, test = data_test,
cl = cl_train[,1, drop = TRUE], k = k_nn)
yhat_star1 <- data.frame(yhat_star)
cv_err_vec[i] = sum(yhat_star1 != cl_test)/30
}
class <- knn(train = train, test = train, cl = cl, k = k_nn)
cv_err <- mean(cv_err_vec)
output <- list(class,cv_err)
return(output)
}
# my_knn_cv <- function(train, cl, k_nn, k_cv){
#   set.seed(302)
#   n <- nrow(train)
#   # Split data in k_cv parts randomly
#   fold <- sample(rep(1:k_cv, length = n))
#   data <- data.frame(train, "split" = fold)
#   cl1 <- data.frame(cl,  "split" = fold)
#   cv_err_vec <- rep(NA, k_cv)
#
#   for (i in 1:k_cv) {
#     data_train <- data %>% filter(split != i)
#     data_test <- data %>% filter(split == i)
#     cl_train <- cl1 %>% filter(split != i)
#     cl_test <- cl1 %>% filter(split == i)
#     cl_train$split <- NULL
#     cl_test$split <- NULL
#
#     # predicts output class
#     yhat_star <- knn(train = data_train, test = data_test,
#                      cl = cl_train[,1, drop = TRUE], k = k_nn)
#     #convert to data frame
#     yhat_star.df <- data.frame(yhat_star)
#     cv_err_vec[i] = sum(yhat_star.df != cl_test) / length(cl_test)
#   }
#   class <- knn(train = train, test = train, cl = cl, k = k_nn)
#   cv_err <- mean(cv_err_vec)
#   output <- list("class" = class, "cv_err" = cv_err)
#   return(output)
# }
output_knn1 <- my_knn_cv(train = iris[, -5], cl = iris$Species,
k_nn=1, k_cv = 5)
output_knn5 <- my_knn_cv(train = iris[, -5], cl = iris$Species,
k_nn=5, k_cv = 5)
result_cv_err <- cbind(output_knn1[[2]], output_knn5[[2]])
colnames(result_cv_err) <- c("CV error (knn = 1)", "CV error (knn = 5)")
result_cv_err
# the cv error of k_nn = 5 is lower and cv error of k_nn = 1 is higher.
output_knn1[[1]]
output_knn5[[1]]
iris$Species
# the training set error of k_nn = 1 is 0 and the training set
# error of k_nn = 5 is higher.
library(dplyr)
library(class)
my_knn_cv <- function(train, cl, k_nn, k_cv){
set.seed(302)
fold <- sample(rep(1:k_cv, length = 150))
data <- data.frame(train, "split" = fold)
cl1 <- data.frame(cl,  "split" = fold)
my_pred <- matrix(NA, 30, 1)
cv_err_vec <- rep(NA, k_cv)
for (i in 1:k_cv) {
data_train <- data %>% filter(split != i)
data_test <- data %>% filter(split == i)
cl_train <- cl1 %>% filter(split != i)
cl_test <- cl1 %>% filter(split == i)
cl_train$split <- NULL
cl_test$split <- NULL
yhat_star <- knn(train = data_train, test = data_test,
cl = cl_train[,1, drop = TRUE], k = k_nn)
yhat_star1 <- data.frame(yhat_star)
cv_err_vec[i] = sum(yhat_star1 != cl_test)/length(cl_test)
}
class <- knn(train = train, test = train, cl = cl, k = k_nn)
cv_err <- mean(cv_err_vec)
output <- list(class,cv_err)
return(output)
}
# my_knn_cv <- function(train, cl, k_nn, k_cv){
#   set.seed(302)
#   n <- nrow(train)
#   # Split data in k_cv parts randomly
#   fold <- sample(rep(1:k_cv, length = n))
#   data <- data.frame(train, "split" = fold)
#   cl1 <- data.frame(cl,  "split" = fold)
#   cv_err_vec <- rep(NA, k_cv)
#
#   for (i in 1:k_cv) {
#     data_train <- data %>% filter(split != i)
#     data_test <- data %>% filter(split == i)
#     cl_train <- cl1 %>% filter(split != i)
#     cl_test <- cl1 %>% filter(split == i)
#     cl_train$split <- NULL
#     cl_test$split <- NULL
#
#     # predicts output class
#     yhat_star <- knn(train = data_train, test = data_test,
#                      cl = cl_train[,1, drop = TRUE], k = k_nn)
#     #convert to data frame
#     yhat_star.df <- data.frame(yhat_star)
#     cv_err_vec[i] = sum(yhat_star.df != cl_test) / length(cl_test)
#   }
#   class <- knn(train = train, test = train, cl = cl, k = k_nn)
#   cv_err <- mean(cv_err_vec)
#   output <- list("class" = class, "cv_err" = cv_err)
#   return(output)
# }
output_knn1 <- my_knn_cv(train = iris[, -5], cl = iris$Species,
k_nn=1, k_cv = 5)
output_knn5 <- my_knn_cv(train = iris[, -5], cl = iris$Species,
k_nn=5, k_cv = 5)
result_cv_err <- cbind(output_knn1[[2]], output_knn5[[2]])
colnames(result_cv_err) <- c("CV error (knn = 1)", "CV error (knn = 5)")
result_cv_err
# the cv error of k_nn = 5 is lower and cv error of k_nn = 1 is higher.
output_knn1[[1]]
output_knn5[[1]]
iris$Species
# the training set error of k_nn = 1 is 0 and the training set
# error of k_nn = 5 is higher.
library(dplyr)
library(class)
my_knn_cv <- function(train, cl, k_nn, k_cv){
set.seed(302)
fold <- sample(rep(1:k_cv, length = 150))
data <- data.frame(train, "split" = fold)
cl1 <- data.frame(cl,  "split" = fold)
my_pred <- matrix(NA, 30, 1)
cv_err_vec <- rep(NA, k_cv)
for (i in 1:k_cv) {
data_train <- data %>% filter(split != i)
data_test <- data %>% filter(split == i)
cl_train <- cl1 %>% filter(split != i)
cl_test <- cl1 %>% filter(split == i)
cl_train$split <- NULL
cl_test$split <- NULL
yhat_star <- knn(train = data_train, test = data_test,
cl = cl_train[,1, drop = TRUE], k = k_nn)
yhat_star1 <- data.frame(yhat_star)
cv_err_vec[i] = sum(yhat_star1 != cl_test)/nrow(cl_test)
}
class <- knn(train = train, test = train, cl = cl, k = k_nn)
cv_err <- mean(cv_err_vec)
output <- list(class,cv_err)
return(output)
}
# my_knn_cv <- function(train, cl, k_nn, k_cv){
#   set.seed(302)
#   n <- nrow(train)
#   # Split data in k_cv parts randomly
#   fold <- sample(rep(1:k_cv, length = n))
#   data <- data.frame(train, "split" = fold)
#   cl1 <- data.frame(cl,  "split" = fold)
#   cv_err_vec <- rep(NA, k_cv)
#
#   for (i in 1:k_cv) {
#     data_train <- data %>% filter(split != i)
#     data_test <- data %>% filter(split == i)
#     cl_train <- cl1 %>% filter(split != i)
#     cl_test <- cl1 %>% filter(split == i)
#     cl_train$split <- NULL
#     cl_test$split <- NULL
#
#     # predicts output class
#     yhat_star <- knn(train = data_train, test = data_test,
#                      cl = cl_train[,1, drop = TRUE], k = k_nn)
#     #convert to data frame
#     yhat_star.df <- data.frame(yhat_star)
#     cv_err_vec[i] = sum(yhat_star.df != cl_test) / length(cl_test)
#   }
#   class <- knn(train = train, test = train, cl = cl, k = k_nn)
#   cv_err <- mean(cv_err_vec)
#   output <- list("class" = class, "cv_err" = cv_err)
#   return(output)
# }
output_knn1 <- my_knn_cv(train = iris[, -5], cl = iris$Species,
k_nn=1, k_cv = 5)
output_knn5 <- my_knn_cv(train = iris[, -5], cl = iris$Species,
k_nn=5, k_cv = 5)
result_cv_err <- cbind(output_knn1[[2]], output_knn5[[2]])
colnames(result_cv_err) <- c("CV error (knn = 1)", "CV error (knn = 5)")
result_cv_err
# the cv error of k_nn = 5 is lower and cv error of k_nn = 1 is higher.
output_knn1[[1]]
output_knn5[[1]]
iris$Species
# the training set error of k_nn = 1 is 0 and the training set
# error of k_nn = 5 is higher.
my_knn_cv(train = iris[, -5], cl = iris$Species, k_nn = 1, k_cv = 5)
@import class
my_pow <- function(x, power = 2) {
return(x^power)
}
f_to_c <- function(temp_F) {
temp_C <- (temp_F - 32) * 5 / 9
return(temp_C)
}
@export
@import class
@export
my_knn_cv <- function(train, cl, k_nn, k_cv){
set.seed(302)
n <- nrow(train)
# Split data in k_cv parts randomly
fold <- sample(rep(1:k_cv, length = n))
data <- data.frame(train, "split" = fold)
cl1 <- data.frame(cl,  "split" = fold)
cv_err_vec <- rep(NA, k_cv)
for (i in 1:k_cv) {
data_train <- data %>% filter(split != i)
data_test <- data %>% filter(split == i)
cl_train <- cl1 %>% filter(split != i)
cl_test <- cl1 %>% filter(split == i)
cl_train$split <- NULL
cl_test$split <- NULL
# predicts output class
yhat_star <- knn(train = data_train, test = data_test,
cl = cl_train[,1, drop = TRUE], k = k_nn)
#convert to data frame
# yhat_star.df <- data.frame(yhat_star)
cv_err_vec[i] = sum(yhat_star.df != cl_test) / nrow(cl_test)
}
class <- knn(train = train, test = train, cl = cl, k = k_nn)
cv_err <- mean(cv_err_vec)
output <- list("class" = my_class, "cv_err" = cv_err)
return(output)
}
#' \item{continent}{continent name}
#' \item{year}{from 1952 to 2007}
#' \item{lifeExp}{life expectancy at birth}
#' \item{pop}{total population}
#' \item{gdpPercap}{per-capita GDP}
#' }
#'
#' \url{https://github.com/jennybc/gapminder}
#'
#' The data were collected by Jennifer Bryan.
"my_gapminder"
my_gapminder
?my_gapminder
?my_gapminder
??my_gapminder
source('~/Desktop/STAT302/projects/project3/stat302package/R/my_gapminder.R')
my_gapminder
?my_gapminder
source('~/Desktop/STAT302/projects/project3/stat302package/R/my_gapminder.R')
source('~/Desktop/STAT302/projects/project3/stat302package/R/my_rf_cv.R')
source('~/Desktop/STAT302/projects/project3/stat302package/R/my_rf_cv.R')
@import class dplyr
@export
my_knn_cv <- function(train, cl, k_nn, k_cv){
set.seed(302)
n <- nrow(train)
# Split data in k_cv parts randomly
fold <- sample(rep(1:k_cv, length = n))
data <- data.frame(train, "split" = fold)
cl1 <- data.frame(cl,  "split" = fold)
cv_err_vec <- rep(NA, k_cv)
for (i in 1:k_cv) {
data_train <- data %>% dplyr::filter(split != i)
data_test <- data %>% dplyr::filter(split == i)
cl_train <- cl1 %>% dplyr::filter(split != i)
cl_test <- cl1 %>% dplyr::filter(split == i)
cl_train$split <- NULL
cl_test$split <- NULL
# predicts output class
yhat_star <- knn(train = data_train, test = data_test,
cl = cl_train[,1, drop = TRUE], k = k_nn)
#convert to data frame
# yhat_star.df <- data.frame(yhat_star)
cv_err_vec[i] = sum(yhat_star.df != cl_test) / nrow(cl_test)
}
class <- knn(train = train, test = train, cl = cl, k = k_nn)
cv_err <- mean(cv_err_vec)
output <- list("class" = my_class, "cv_err" = cv_err)
return(output)
}
#'
#' @return a list with a numeric with the cross-validation error
#'
#' @examples
#' my_rf_cv(5)
#'
#' @import class randomForest dplyr
#'
#' @export
#'
my_rf_cv <- function(k){
set.seed(229)
n <- nrow(my_gapminder)
fold <- sample(rep(1:k, length = n))
data <- data.frame(my_gapminder, "split" = fold)
cv_err_vec <- rep(NA, k)
for (i in 1:k) {
data_train <- data %>% dplyr::filter(split != i)
data_test <- data %>% dplyr::filter(split == i)
model <- randomForest(lifeExp ~ gdpPercap, data = data_train, ntree = 100)
prediction <- predict(my_model, data_test[, -1])
cv_err_vec[i] = mean((data_test$lifeExp - prediction)^2)
}
cv_err <- mean(cv_err_vec)
return(cv_err)
}
devtools::document()
devtools::document()
usethis::use_data(my_gapminder)
#' \item{gdpPercap}{per-capita GDP}
#' }
#'
#' \source
#' https://github.com/jennybc/gapminder
#'
#'#' @examples
#' head(my_gapminder)
#'
#' The data were collected by Jennifer Bryan.
"my_gapminder"
usethis::use_data(my_gapminder)
data(gapminder)
library(gapminder)
data("gapminder")
my_gapminder <- gapmminder
data(gapminder)
my_gapminder <- gapminder
usethis::use_data(my_gapminder)
usethis::use_test("my_knn_cv")
source('~/Desktop/STAT302/projects/project3/stat302package/R/my_rf_cv.R')
stop("k_nn and k_cv must be numeric !")
if(!is.numeric(k_nn) || !is.numeric(k_cv)) {
stop("k_nn and k_cv must be numeric !")
}
set.seed(302)
n <- nrow(train)
# Split data in k_cv parts randomly
fold <- sample(rep(1:k_cv, length = n))
data <- data.frame(train, "split" = fold)
cl1 <- data.frame(cl,  "split" = fold)
cv_err_vec <- rep(NA, k_cv)
for (i in 1:k_cv) {
data_train <- data %>% dplyr::filter(split != i)
data_test <- data %>% dplyr::filter(split == i)
cl_train <- cl1 %>% dplyr::filter(split != i)
cl_test <- cl1 %>% dplyr::filter(split == i)
cl_train$split <- NULL
cl_test$split <- NULL
# predicts output class
yhat_star <- knn(train = data_train, test = data_test,
cl = cl_train[,1, drop = TRUE], k = k_nn)
#convert to data frame
# yhat_star.df <- data.frame(yhat_star)
cv_err_vec[i] = sum(yhat_star.df != cl_test) / nrow(cl_test)
}
#' k-Nearest Neighbors Cross-Validation
#'
#' This function is to to predict the output class using covariates.
#'
#' @param train input data frame
#' @param cl true class value of your training data
#' @param k_nn integer representing the number of neighbors
#' @param k_cv integer representing the number of folds
#' @keywords prediction
#'
#' @return A list with a vector of the predicted class for all observations and
#'  a numeric with the cross-validation misclassification error
#'
#' @examples
#' my_knn_cv(train = iris[, -5], cl = iris$Species, k_nn = 1, k_cv = 5)
#' my_knn_cv(train = iris[, -5], cl = iris$Species, k_nn = 5, k_cv = 5)
#'
#' @import class dplyr
#' @export
my_knn_cv <- function(train, cl, k_nn, k_cv){
if(!is.numeric(k_nn) || !is.numeric(k_cv)) {
stop("k_nn and k_cv must be numeric !")
}
set.seed(302)
n <- nrow(train)
# Split data in k_cv parts randomly
fold <- sample(rep(1:k_cv, length = n))
data <- data.frame(train, "split" = fold)
cl1 <- data.frame(cl,  "split" = fold)
cv_err_vec <- rep(NA, k_cv)
for (i in 1:k_cv) {
data_train <- data %>% dplyr::filter(split != i)
data_test <- data %>% dplyr::filter(split == i)
cl_train <- cl1 %>% dplyr::filter(split != i)
cl_test <- cl1 %>% dplyr::filter(split == i)
cl_train$split <- NULL
cl_test$split <- NULL
# predicts output class
yhat_star <- knn(train = data_train, test = data_test,
cl = cl_train[,1, drop = TRUE], k = k_nn)
#convert to data frame
# yhat_star.df <- data.frame(yhat_star)
cv_err_vec[i] = sum(yhat_star.df != cl_test) / nrow(cl_test)
}
class <- knn(train = train, test = train, cl = cl, k = k_nn)
cv_err <- mean(cv_err_vec)
output <- list("class" = my_class, "cv_err" = cv_err)
return(output)
}
usethis::use_test("my_rf_cv")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(stat302package)
usethis::use_testthat()
usethis::use_travis()
use_travis()
devtools::check()
#' \item{gdpPercap}{per-capita GDP}
#' }
#'
#' \source
#' https://github.com/jennybc/gapminder
#'
#'#' @examples
#' head(my_gapminder)
#'
#' The data were collected by Jennifer Bryan.
"my_gapminder
devtools::check()
#' \item{gdpPercap}{per-capita GDP}
#' }
#'
#' \source
#' https://github.com/jennybc/gapminder
#'
#'#' @examples
#' head(my_gapminder)
#'
#' The data were collected by Jennifer Bryan.
"my_gapminder"
devtools::check()
devtools::check()
source('~/Desktop/STAT302/projects/project3/stat302package/R/my_rf_cv.R')
devtools::check()
devtools::check()
usethis::use_travis()
usethis::use_travis()
usethis::use_coverage()
